from typing import Optional, Type, Any, TypeVar
from datetime import datetime,timedelta
import fnmatch, subprocess, pathlib
from pyspark import SparkContext
from pyspark.sql import SparkSession, DataFrame, DataFrameWriter, dataframe

T = TypeVar('T', bound=DataFrame|DataFrameWriter)


def parse_default_arg(f: T,default_conf,sparkSess: SparkSession= None) -> T:
    for conf in default_conf:
        val = set_spark_conf(*conf[0])
        if not val: val = conf[2]
        argFunc = conf[1]
        f = praseArg(f,argFunc(val))
    return f

def set_spark_conf(values={},sparkSess=None) -> None :
    if not sparkSess:
        sparkSess = SparkSession.getActiveSession()
    list(sparkSess.conf.set(key, value) for key, value in values.items())

def get_spark_conf(param,defaultVal=None,sparkSess=None) -> Optional[str] :
    if not sparkSess:
        sparkSess = SparkSession.getActiveSession()
    return sparkSess.conf.get(param,defaultVal)

def get_dbutils(spark: SparkSession) -> Optional[Type]:
    try:
        from pyspark.dbutils import DBUtils
        return DBUtils(spark)
    except ImportError:
        return None

def get_sercet(dbutils: callable,scope: str, key:str ) -> Optional[str]:
    if dbutils:
        return dbutils.secrets.get(scope, key)
    return None

def get_catalog_name(in_str: str = "") -> str:
    """

    Args:
        in_str (str, optional): _description_. Defaults to "".

    Returns:
        str: _description_
    """
    match in_str:
        case "prod": return "prod"
        case "staging": return "test"
        case _: return "dev"

def get_active_branch_name() -> str:
    """extract current git branch name

    Returns:
        str: _description_
    """
    res = subprocess.run('git rev-parse --abbrev-ref HEAD',stdout=subprocess.PIPE, shell=True)
    return res.stdout.decode().replace('\n', '')


def praseArg(func: T, conf: dict)->T:
    for k,v in conf.items():
        if type(v) == dict:
            func = addAction(func,k,**v)
        elif type(v) == list:
            func = addAction(func,k,*v)
        else:
            func = addAction(func,k,v)
    return func

def addAction(func: T, option:str,*arg:Any,**kwargg:Any) -> T:
    func = getattr(func,option)
    return func(*arg,**kwargg)


def get_task_entrypoints(project_name: str) -> list[str]:
    """
    Returns a list of task entrypoints to be used by setuptools.

    The list is generated by scanning the tasks directory for python files
    and returning the name of the file without the .py extension.

    Example entrypoint: "bronze.web_sales_extract_bronze_task=paofactory_edp.tasks.bronze.web_sales_extract_bronze_task:main"

    Args:
        project_name (str): The name of the project.

    Returns:
        list[str]: A list of task entrypoints.
    """
    task_entrypoints = []

    tasks_path = f"src/{project_name}/tasks/"

    for task in pathlib.Path(tasks_path).glob("**/*.py"):
        print(task)
        if task.name == "__init__.py":
            continue

        middle = (
            (str(task).replace(tasks_path, ""))
            .replace(f"/{task.name}", "")
            .replace("/", ".")
        )

        print(middle)
        task_entrypoints.append(
            f"{middle}.{task.stem}={project_name}.tasks.{middle}.{task.stem}:main"
        )
    return task_entrypoints

def tableExists(query_str:str ,sparkSess: SparkSession=None) -> bool:
    """as named

    Args:
        query_str (str): _description_
        sparkSess (str, optional): _description_. Defaults to None.

    Returns:
        bool: _description_
    """
    if not sparkSess:
        sparkSess = SparkSession.getActiveSession()
    return sparkSess.sql(query_str).count() > 0


def getTableExistsQuery(table: str,schema: str,catalog: str=None) -> str:
    """generate the sql query for lookup information schema for check table exists or not

    Args:
        table (str): _description_
        schema (str): _description_. Defaults to None.
        catalog (str, optional): _description_. Defaults to None.

    Returns:
        str: _description_
    """
    info_table = "information_schema.tables"
    if catalog:
        info_table = f"{catalog}.{info_table}"
    return f"""SELECT 1 
                FROM {info_table} 
                WHERE table_name = '{table}'
                AND table_schema = '{schema}' LIMIT 1"""
       

def getProperty(ins , p_name, defaultVal = None) -> Optional[any]:
    """_summary_

    Args:
        ins (_type_): _description_
        p_name (_type_): _description_

    Returns:
        Optional[any]: _description_
    """
    return getattr(ins, p_name) if hasattr(ins, p_name) else defaultVal


def archive_files(*args: Any, **kwargs: Any) -> None:
    processed_files = kwargs.get("processed_files")
    internal_storage = kwargs.get("internal_storage")
    curDate = datetime.now()
    curDateStr = curDate.strftime("%Y%m%d")
    archive_folder = f"archive/{curDateStr}/"
    # file_list = internal_storage.get_file_list(processed_files)
    file_list = internal_storage.get_file_list()
    def filter_logic(x):
        return fnmatch.fnmatch(x['name'],f"*{processed_files}") \
        and not fnmatch.fnmatch(x['name'],f"{internal_storage.get_directory()}archive*")
    # filter_name = lambda x : fnmatch.fnmatch(x['name'],f"*{processed_files}")
    filename_list = filter(filter_logic,file_list)
    for updated_file in filename_list:
        file_name = updated_file["name"]
        archive_file_name = file_name.replace(internal_storage.get_directory(),f"{archive_folder}")
        file_bytes = internal_storage.get_file(file_name.replace(internal_storage.get_directory(),""))
        internal_storage.put_file(archive_file_name,file_bytes)
        internal_storage.delete_file(file_name.replace(internal_storage.get_directory(),""))
    cleanupDate = curDate - timedelta(days=365)
    cleanupDateStr = cleanupDate.strftime("%Y%m%d")
    archFolderList = internal_storage.get_file_list("archive")
    for obj in archFolderList:
        match obj["is_directory"]:
            case True:
                tmpFolderName = obj["name"]
                checkName = tmpFolderName.replace(f"{internal_storage.get_directory()}archive/",'')
                if int(cleanupDateStr) > int(checkName):
                    file_system_client = internal_storage.conn.get_file_system_client(file_system=internal_storage.get_file_system())
                    directory_client = file_system_client.get_directory_client(
                    internal_storage.get_directory()+f"archive/{checkName}/")
                    directory_client.delete_directory()
